{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weMGj-eFXQ8d","executionInfo":{"status":"ok","timestamp":1691295358514,"user_tz":-330,"elapsed":17201,"user":{"displayName":"Saumajit Saha","userId":"15175769905233503939"}},"outputId":"0a8761eb-42cd-4482-f1c6-552de1c62c14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJGTQsR7X69c","executionInfo":{"status":"ok","timestamp":1691295366060,"user_tz":-330,"elapsed":2944,"user":{"displayName":"Saumajit Saha","userId":"15175769905233503939"}},"outputId":"9f656ae0-6820-49db-845c-1ff7a4b60703"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## **Importing packages**"],"metadata":{"id":"cl-6l6cxeMi1"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap\n","from tqdm import tqdm\n","from tqdm import trange\n","import os\n","\n","# Torch ML libraries\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Misc.\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"gynL1ysKX9SG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Reading data files**"],"metadata":{"id":"cUz7t4sneREJ"}},{"cell_type":"code","source":["train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BLP2023/blp_task2/data/blp23_sentiment_train.tsv', sep = '\\t')\n","dev_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BLP2023/blp_task2/data/blp23_sentiment_dev.tsv', sep='\\t')\n","test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BLP2023/blp_task2/data/blp23_sentiment_dev_test.tsv', sep='\\t')"],"metadata":{"id":"F1n3prZ6X_98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_texts = train_data['text'].tolist()\n","dev_texts = dev_data['text'].tolist()\n","\n","train_label = train_data['label'].tolist()\n","dev_label = dev_data['label'].tolist()"],"metadata":{"id":"GfjDDMO9YC30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_to_id = {'Positive' : 1, 'Neutral': 0, 'Negative': 2}\n","id_to_label = {k: v for v, k in label_to_id.items()}\n","print(id_to_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUwa7s7gYFSi","executionInfo":{"status":"ok","timestamp":1691295388557,"user_tz":-330,"elapsed":27,"user":{"displayName":"Saumajit Saha","userId":"15175769905233503939"}},"outputId":"c9031668-129b-4f72-f27f-02b2de768c77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'Positive', 0: 'Neutral', 2: 'Negative'}\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","max_len = 100\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azPe9lmHY9eE","executionInfo":{"status":"ok","timestamp":1691295388558,"user_tz":-330,"elapsed":23,"user":{"displayName":"Saumajit Saha","userId":"15175769905233503939"}},"outputId":"4257efdc-02d0-47a6-fbef-efb7fe71a826"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")"],"metadata":{"id":"paW7gvfqZBje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BertInputItem(object):\n","  def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n","    self.text = text\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.label_id = label_id\n","\n","def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):\n","  input_items = []\n","  examples = zip(example_texts, example_labels)\n","  for (ex_index, (text, label)) in enumerate(examples):\n","\n","    input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n","    if len(input_ids) > max_seq_length:\n","      input_ids = input_ids[:max_seq_length]\n","\n","    segment_ids = [0]*len(input_ids)\n","    input_mask = [1]*len(input_ids)\n","    padding = [0]*(max_seq_length - len(input_ids))\n","    input_ids += padding\n","    input_mask += padding\n","    segment_ids += padding\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    label_id = label2idx[label]\n","\n","    input_items.append(BertInputItem(text = text, input_ids = input_ids, input_mask = input_mask, segment_ids = segment_ids, label_id = label_id))\n","  return input_items"],"metadata":{"id":"dKWmBspzZVC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Preparing input to model**"],"metadata":{"id":"L0LQsOhpeiq9"}},{"cell_type":"code","source":["train_features = convert_examples_to_inputs(train_texts, train_label, label_to_id, max_len, tokenizer, verbose=0)\n","dev_features = convert_examples_to_inputs(dev_texts, dev_label, label_to_id, max_len, tokenizer, verbose=0)"],"metadata":{"id":"qHmGufJVZYNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n","\n","def get_data_loader(features, max_seq_length, batch_size, shuffle=True):\n","  all_input_ids = torch.tensor([f.input_ids for f in features], dtype = torch.long)\n","  all_input_mask = torch.tensor([f.input_mask for f in features], dtype = torch.long)\n","  all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype = torch.long)\n","  all_label_ids = torch.tensor([f.label_id for f in features], dtype = torch.long)\n","  data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","\n","  dataloader = DataLoader(data, shuffle=shuffle, batch_size = batch_size)\n","  return dataloader"],"metadata":{"id":"MzHBn1nsZbH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = get_data_loader(train_features, max_len, BATCH_SIZE, shuffle=True)\n","dev_dataloader = get_data_loader(dev_features, max_len, BATCH_SIZE, shuffle=False)"],"metadata":{"id":"SCH4vU2kZdub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Initializing model**"],"metadata":{"id":"HLDQeyoae9Rs"}},{"cell_type":"code","source":["from transformers.models.bert.modeling_bert import BertForSequenceClassification\n","model = BertForSequenceClassification.from_pretrained(\"sagorsarker/bangla-bert-base\", num_labels = len(label_to_id))\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-P8NWzEJZf_d","executionInfo":{"status":"ok","timestamp":1691295529375,"user_tz":-330,"elapsed":9172,"user":{"displayName":"Saumajit Saha","userId":"15175769905233503939"}},"outputId":"e9c1a99f-94d7-4df0-9edf-29d78a58326e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/BLP2023/Saved_dir/Bangla_bert_pretraining/pretrained-bert and are newly initialized: ['bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Hyperparameter definition"],"metadata":{"id":"96A5CjNLfB_d"}},{"cell_type":"code","source":["from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n","import math\n","\n","\n","GRADIENT_ACCUMULATION_STEPS = 1\n","EPOCHS = 15\n","LEARNING_RATE = 5 * 10**-5\n","WARMUP_PROPORTION = 0.1\n","MAX_GRAD_NORM = 5\n","\n","num_train_steps = int(len(train_dataloader.dataset)/BATCH_SIZE/GRADIENT_ACCUMULATION_STEPS * EPOCHS)\n","num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","\n","optimizer_grouped_parameters = [\n","    {'params' : [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params' : [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 00.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr = LEARNING_RATE, correct_bias = False)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_train_steps)"],"metadata":{"id":"y2oguufMZl18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, dataloader):\n","  model.eval()\n","\n","  eval_loss = 0\n","  nb_eval_steps = 0\n","  predicted_labels, correct_labels = [], []\n","\n","  for step, batch in enumerate(tqdm(dataloader, desc = \"Evaluation iteration\")):\n","      batch = tuple(t.to(device) for t in batch)\n","      input_ids, input_mask, segment_ids, label_ids = batch\n","\n","      with torch.no_grad():\n","        outputs = model(input_ids, attention_mask = input_mask, token_type_ids = segment_ids, labels=label_ids)\n","      tmp_eval_loss = outputs[0]\n","      logits = outputs[1]\n","      outputs = np.argmax(logits.to('cpu'), axis=1)\n","      label_ids = label_ids.to('cpu').numpy()\n","\n","      predicted_labels += list(outputs)\n","      correct_labels += list(label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      nb_eval_steps += 1\n","\n","  eval_loss = eval_loss/nb_eval_steps\n","  correct_labels = np.array(correct_labels)\n","  predicted_labels = np.array(predicted_labels)\n","\n","\n","  return eval_loss, correct_labels, predicted_labels\n"],"metadata":{"id":"5RN5Cvfnai9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/BLP2023/Saved_dir/Bangla_bert_pretraining/FT_task2'\n","MODEL_NAME = \"/content/drive/MyDrive/Colab Notebooks/BLP2023/Saved_dir/Bangla_bert_pretraining/pretrained-bert\"\n","MODEL_FILE_NAME = 'pytorch_model.bin'\n","PATIENCE = 2"],"metadata":{"id":"98CX9d-palJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training loop**"],"metadata":{"id":"cpXBFKSefHcG"}},{"cell_type":"code","source":["train_loss_history = []\n","dev_loss_history = []\n","\n","no_improvement = 0\n","for e in trange(int(EPOCHS), desc=\"Epoch\"):\n","    model.train()\n","    tr_loss = 0\n","\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, input_mask, segment_ids, label_ids = batch\n","\n","        outputs = model(input_ids, attention_mask = input_mask, token_type_ids = segment_ids, labels = label_ids)\n","\n","        loss = outputs[0]\n","\n","        if GRADIENT_ACCUMULATION_STEPS > 1:\n","            loss = loss/GRADIENT_ACCUMULATION_STEPS\n","\n","        loss.backward()\n","        tr_loss += loss.item()\n","\n","        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n","            torch.nn.utils.clip_grad_norm(model.parameters(), MAX_GRAD_NORM)\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","    dev_loss, _, _ = evaluate(model, dev_dataloader)\n","\n","    print(\"Loss history : \", dev_loss_history)\n","    #print(\"Dev loss : \", dev_loss)\n","\n","    if len(dev_loss_history) == 0 or dev_loss < min(dev_loss_history):\n","        no_improvement = 0\n","        model_to_save = model.module if hasattr(model, 'module') else model\n","        output_model_dir = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n","        torch.save(model_to_save.state_dict(), output_model_dir)\n","        print(f\"Model saved at epoch {e}\")\n","    else:\n","        no_improvement += 1\n","\n","    if no_improvement >= PATIENCE:\n","        print(\"No improvement on development set. Finish training\")\n","        break\n","\n","    dev_loss_history.append(dev_loss)\n","    train_loss_history.append(tr_loss/len(train_dataloader))\n","\n","\n"],"metadata":{"id":"bkUR-2-va0xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train-Val loss plot"],"metadata":{"id":"8FAEGc5sfMZa"}},{"cell_type":"code","source":["plt.plot(train_loss_history, label='train_loss')\n","plt.plot(dev_loss_history, label='validation loss')\n","\n","plt.title('Training history')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()"],"metadata":{"id":"U3jgYCksa5PZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculating metrics for evaluation"],"metadata":{"id":"BlGzxlRafRvM"}},{"cell_type":"code","source":["import os\n","from sklearn.metrics import classification_report, precision_recall_fscore_support\n","\n","model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc:storage)\n","model = BertForSequenceClassification.from_pretrained(MODEL_NAME, state_dict=model_state_dict, num_labels = len(label_to_id))\n","model.to(device)\n","\n","model.eval()\n","\n","#_, train_correct, train_predicted = evaluate(model, train_dataloader)\n","_, dev_correct, dev_predicted = evaluate(model, dev_dataloader)\n","\n","#print(\"Training performance : \", precision_recall_fscore_support(train_correct, train_predicted, average='micro'))\n","print(\"Dev performance : \", precision_recall_fscore_support(dev_correct, dev_predicted, average='micro'))\n","\n","from sklearn.metrics import f1_score\n","\n","print(\"Micro F1-score = \", f1_score(dev_correct, dev_predicted, average='micro'))"],"metadata":{"id":"LCeteo54a90y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GfHM03VrbEca"},"execution_count":null,"outputs":[]}]}